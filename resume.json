{
  "basics": {
    "name": "Matthew Dantas-McCutcheon",
    "label": "Software Engineer",
    "picture": "",
    "email": "mateusdm88 at gmail dot com",
    "summary": "Software engineer, problem-solver, rapid learner, leader and individual contributor. I am interested in expanding on my experience handling and processing internet-scale traffic. I also like math and functional programming; I've recently married these interests in learning the proof-assistant language, Coq.",
    "profiles": [
      {
        "network": "Github",
        "username": "mDantas",
        "url": "https://github.com/mDantas"
      }
    ]
  },
  "work": [
    {
      "company": "Career Break",
      "startDate": "2021-05-01",
      "endDate": "Present",
      "summary": "I've taken time off to run my daughter's virtual schooling and spend more time with her. More recently I've started also doing some self-study. This has included: (a) starting to learn abstract algebra (following Paolo Aluffi's book, Algebra: Chapter 0), (b) solving ProjectEuler problems in Haskell, and (c) learning the proof assistant, Coq, via the Software Foundations series."
    },
    {
      "company": "Truebill",
      "position": "Data & Infrastructure Engineer",
      "startDate": "2019-11-01",
      "endDate": "2021-05-01",
      "summary": "I made sundry infrastructure and data engineering contributions, chiefly around exporting/synchronizing trillions
      of records from Postgres to columnar datawarehouses.",
      "highlights": [
        "Developed a solution to get the company's 2.5 trillion transactions batch-synchronized from Postgres (in AWS) to BigQuery.",
        "Deep dived into Clickhouse to evaluate it's suitability as a replacement for BigQuery. The output of this investigation
        included a proof-of-concept 3-node Clickhouse cluster following our transactions table in Postgres (streaming updates)."
      ]
    },
    {
      "company": "Career Break",
      "startDate": "2019-05-24",
      "endDate": "2019-11-01",
      "summary": "I took some time off. I enjoyed time with family. I also volunteered at my daughter's pre-school where I did read-alouds twice a week."
    },
    {
      "company": "Megaphone (formerly Panoply Media)",
      "position": "Infrastructure Team Lead",
      "startDate": "2018-03-01",
      "endDate": "2019-05-24",
      "summary": "I productionalized the company's data platform and became the data pipeline owner. I implemented push-button deployment of the entire data pipeline while increasing overall availability, monitoring, and correctness. As for scale, when I left, Megaphone was serving ~350MM downloads per month accounting for over half of all global podcast downloads. This corresponded to ~50 petabytes (mostly CDN-fronted) of downloads per month, and the data pipeline processing approximately 120MM events (podcast client requests) per day. I also led the team (2-4 direct reports) responsible for adserving, content stitching, and ad inventory forecasting.",
      "highlights": [
				"Established the company's Infrastructure-as-Code standard. After a process of prototyping other Infrastructure-as-Code solutions (e.g. Kubernetes), I discovered Terraform and Packer and proceeded to capture our Druid cluster there. This meant that for the first time we could incrementally develop, test, and version the company's analytics backbone. I also introduced Stackdriver monitoring so that we had pipeline health metrics tied to Pagerduty. This spanned my first few months at Megaphone.",
				"Architected the expansion of the data platform to support 3 (up from 1) different download-counting methodologies. This work spanned ~1.5 months and was implemented by myself and another engineer. In this effort, I learned the Apache Dataflow Java SDK sufficiently to make significant updates to the existing Dataflow pipeline. The verification process involved loading the new pipeline's output into BigQuery to make extensive quality checks. The effort landed successfully in production early on 12/15/19, two weeks before the Jan 1 deadline.",
				"Architected the migration of the pipeline's source-data-archiving responsibility from a cron-triggered Ruby script whose output was manually verified in email form by our SRE to a Stackdriver-monitored service consisting of serverless functions, queues, and cloud storage.",
				"Architected and implemented the migration of a brittle Kubernetes-hosted Go service that enhanced incoming log data to Dataflow. The Go service had become brittle under increased load and had begun to crash resulting in daily data losses of 10% on average. The migration to Dataflow jobs orchestrated by message queues and cloud-functions ended this pattern of data loss.",
        "Built the BI data pipeline: wrote a ~300 LOC bash script that replicated data from our application database (Postgres) and our timeseries downloads database (Druid) to BigQuery and then checked correctness (checking just counts between Druid and BigQuery but column-MD5 checks for postgres data). I also prototyped moving this to Apache Airflow but the script worked too well to justify the move. This powered all of BI reporting.",
        "Built one half of the ad-inventory forecasting platform. I initially prototyped in Go but then moved to Haskell as a much more natural language for solving this combinatorically-focused problem. Our forecasting platform achieved 'first-light' but I don't know whether it went further after my departure.",
				"Frequent one-off efforts to support BI in the form of data-product generation, spelunking into platform anomalies, requests from finance, etc. This mostly took the form of shell scripting on high-CPU count VMs and loading the resulting output into BigQuery for analysis.",
				"Rapid prototyping of tools and technologies to test their fit against our needs. These included Apache Hive and Kubernetes for our Druid cluster."
      ]
    },
    {
      "company": "Vistaprint",
      "position": "Software Engineer, Vistaprint Data Warehouse",
      "startDate": "2017-10-01",
      "endDate": "2018-03-01",
      "summary": "As part of a two person team, I ETL'd Vistaprint Digital's subscription data from disparate relational sources (~12 products across two billing platforms) into a central data warehouse. My contribution was co-creating and refining the target fact and dimension tables as well as the sprocs that populate them. Most of the work was in Azure Data Warehouse. The data was used to forecast, per-product, KPIs like signups, bookings, cancellations, etc.",
      "highlights": [
        "Normalizing data from relational format to columnar-stored fact/dimension tables (star schema).",
        "Using change-tracking-replication infrastructure to copy data over for processing."
      ]
    },
    {
      "company": "Vistaprint",
      "position": "Software Engineer, Local Listings product",
      "website": "www.vistaprint.com/local-listings.aspx",
      "startDate": "2015-04-01",
      "endDate": "2017-10-01",
      "summary": "Local Listings is a tool for small businesses to disseminate their contact information to many directories so customers can find them. My technical contributions spanned the Postgres/Express/React/Node stack with a focus on the backend. I was also scrum master for my team and worked closely with the PO on product decisions. I remain on the on-call rotation for this product.",
      "highlights": [
        "Principal contributor to app codebase.",
        "Led a team of 3 engineers and 2 QE in migrating to a new 3rd-party backend service provider. In numbers: 4 months of effort, ~100 Jira tickets covering the entire stack, ~40k existing users affected (in addition to all new signups).",
        "Refactored the database schema from update to event-driven.",
        "Introduced and implemented a means to measure customer impact: (1) call Google's Custom Search Engine API to search for users' identifying information and (2) process resulting JSON to fuzzy search for the user's presence in results. Track changes in search result appearances over time to determine whether Local Listings improves users' appearance in organic search results.",
        "A (small) open source contribution - Augmented an existing Node library's Levenshtein implementation to handle fuzzy searching (https://github.com/NaturalNode/natural/pull/360).",
        "Internationalization (using react-intl). Internationalized application frontend for future expansion to arbitrary locales.",
        "Expanded the application's unit test footprint ~5x."
      ]
    },
    {
      "company": "Vistaprint",
      "position": "Software Engineer, Pagemodo product",
      "website": "http://www.pagemodo.com",
      "startDate": "2014-06-01",
      "endDate": "2015-04-01",
      "summary": "Pagemodo is a one-stop-shop social media marketing tool targeted at small businesses. It is a Rails app with postgres db hosted on Heroku with a mix of Marionette/Backbone & React on the frontend.",
      "highlights": [
        "A Tweet-integrated Marionette Backbone SPA for Social Media Marketing World 2015 (https://www.pagemodo.com/smmw15). This was a thin vertical slice of work, adding tables/models, controller, views, and a Backbone app. Note that the 'post designer' to which I integrated the Twitter-posting experience already existed as a core feature of the Pagemodo product.",
        "A short stint working on our Apache SOLR integration to improve our content recommendation to users.",
        "Sundry bug fixes and small and medium feature tickets across the stack."
      ]
    },
    {
      "company": "Dell @ Executive Office of the President",
      "position": "Rails developer - Max.gov",
      "website": "https://max.gov",
      "startDate": "2013-04-01",
      "endDate": "2014-06-01",
      "summary": "Max.gov is a tool to collect human-entered data across Federal agencies (e.g. to compile the federal budget). When I left, it had ~100k users. Brought on as an individual feature contributor, I eventually became the sole developer as a result of team restructuring. I interacted extensively directly with government clients to script ad-hoc data needs. My main contribution turned out to be, in effect, an ETL written in Rails. This script was needed to migrate individual tree-subsets of user data (order of magnitude, ~100k records) from a legacy database with Latin1 encoding a new database with UTF8 encoding. The following highlights are in reference to this script.",
      "highlights": [
        "Outperformed pre-existing script in speed by up to two orders of magnitude (days to minutes).",
        "Outperformed pre-existing script in correctness - whereas the latter would perform inconsistently deep/shallow copies of data, thoughtful design and testing ensured that this script did not.",
        "Script was used to migrate mission-critical user data on a per-user basis to the new database.",
        "Used binary search to robustly insert a 'haystack' of good data while collecting the 'needles' of bad data (usually records whose Latin1 to UTF8 translation overflowed the column size)",
        "Wrote logic in Ruby to: (1) traverse the application schema (~10 tables) from a given root record, (2) select the entire source tree of data into memory, (3) dynamically create the bulk insertion statements to insert the data into the target database in-order so as to respect relational integrity, and (4) validate integrity of the copy."
      ]
    },
    {
      "company": "Lockheed Martin",
      "position": "Systems Engineer Asc",
      "startDate": "2011-04-01",
      "endDate": "2014-06-01",
      "summary": "Sundry assignments, most relevantly (1) as a Rails dev for 6 months and then (2) writing JasperReports SQL queries for a different project.",
      "highlights": [
        "Developed (as sole and full-stack developer learning on the fly) a web application for placing and reviewing procurement orders following a role-based approval workflow. This was a Ruby on Rails app with MySQL database and vanilla Rails templates and jQuery on the frontend. Tools/tech involved: role-based authorization, authentication, LDAP, cron jobs, SMTP mailer, deterministic finite automata (to implement the state machine for order workflow), CSS3, MySQL",
        "Tested and corrected system-generated Jasper reports",
        "Scripted a domain-specific-language in Python to speed up generation of Jasper reports"
      ]
    },
    {
      "company": "Booz Allen Hamilton",
      "position": "Intern",
      "startDate": "2010-06-01",
      "endDate": "2010-08-01",
      "summary": "BAH's cybersecurity summer internship program",
      "highlights": [
        "Developed software to gather open-source intelligence as part of a 3 person team",
        "Presented to fellow interns and employees on the project's outcome",
        "Skills: web scraping, web crawling, Python"
      ]
    },
    {
      "company": "Institute for Physical Sciences and Technology",
      "position": "Undergraduate Researcher",
      "startDate": "2006-06-01",
      "endDate": "2009-10-01",
      "website": "http://www.ipst.umd.edu/undergraduate/",
      "summary": "Built a numerical model of a time-of-flight (ToF IMS) ion mass spectrometer. ToF IMS are used to measure solar wind composition. For an example diagram, see https://www2.mps.mpg.de/images/projekte/bepicolombo/mppe/mppe10.jpg. Input to the simulator was a specific solar wind composition. Output constituted histograms describing measurements from 3 instrument components. The simulation used lookup-tables of parameters describing these output histograms. These tables were populated using data collected from physics simulation software (SIMION and SRIM) and parametrized with scripts in Microsoft VBA.",
      "highlights": [
        "Independently designed and implemented the simulation (in C) of an instrument measuring solar wind composition.",
        "Designed and prototyped a genetic algorithm in Java to infer solar wind composition from instrument output spectra.",
        "Authored 70 pages documenting design, development, and usage of the simulator."
      ]
    }
  ],
  "education": [
    {
      "institution": "University of Maryland, College Park",
      "area": "Mathematics",
      "studyType": "Bachelor of Science",
      "startDate": "2006-09-01",
      "endDate": "2011-05-01",
      "gpa": "3.6",
      "courses": [
        "Real analysis",
        "Quantum physics",
        "Linear algebra",
        "Combinatorics",
        "Cryptology",
        "Computational methods"
      ]
    },
    {
      "institution": "University of Maryland, College Park",
      "area": "Computer Science",
      "studyType": "Bachelor of Science",
      "startDate": "2006-09-01",
      "endDate": "2011-05-01",
      "gpa": "3.6",
      "courses": [
        "Machine Learning",
        "Algorithms",
        "Systems Architecture"
      ]
    }
  ],
  "skills": [
    {
      "name": "Data Pipeline",
      "level": "Experienced",
      "keywords": [
        "Dataflow (Apache Beam)",
        "BigQuery",
        "ETL/ELT",
        "Redshift",
        "Microsoft PDW",
				"Columnar storage",
				"Timeseries databases (Apache Druid)",
				"Validation checks",
				"Monitoring",
                                "Clickhouse"
      ]
    },
    {
      "name": "Google Cloud",
      "level": "Experienced",
      "keywords": [
        "PubSub",
        "CloudFunction",
        "BigQuery",
        "Compute (VM)",
        "Cloud Scheduler",
        "Dataflow (Apache Beam)",
        "Load balancers",
        "VPC",
        "Storage",
        "Stackdriver"
      ]
    },
    {
      "name": "Infrastructure as Code",
      "level": "Experienced",
      "keywords": [
        "Terraform",
        "Packer"
      ]
    },
    {
      "name": "Ops/Cloud",
      "level": "Experienced",
      "keywords": [
        "Heroku",
        "Logentries",
        "NewRelic",
        "Stackdriver",
        "Pagerduty",
        "Packer",
        "DigitalOcean",
        "AWS"
      ]
    },
    {
      "name": "CDN configuration",
      "level": "Some Experience",
      "keywords": [
        "Origin shielding",
        "Primary/backup",
        "Highwinds/Stackpath (vendor)"
			]
    },
    {
      "name": "Druid Cluster Administration",
      "level": "Experienced",
      "keywords": [
        "Scaling",
        "Configuring",
        "Monitoring",
        "Debugging & Troubleshooting"
      ]
    },
    {
      "name": "Web Development",
      "level": "Experienced",
      "keywords": [
        "Express",
        "Node",
        "React",
        "Rails",
        "Templating",
        "Creating and consuming web APIs",
        "il8n"
      ]
    },
    {
      "name": "Relational databases",
      "level": "Experienced",
      "keywords": [
        "Postgres",
        "DB2",
        "Sql server",
        "Common table expressions",
        "Stored procedures",
        "Views",
        "Indexes",
        "Query inspection/design/optimization"
      ]
    },
    {
      "name": "Software Development Lifecycle",
      "level": "Experienced",
      "keywords": [
        "On-call rotation",
        "Scrum",
        "JIRA",
        "git",
        "scrum master",
        "distributed teams"
      ]
    },
    {
      "name": "Application Testing",
      "level": "Experienced",
      "keywords": [
        "Stubs",
        "Spies",
        "Mocks"
      ]
    },
    {
      "name": "Functional Languages",
      "level": "Some experience",
      "keywords": [
        "Haskell",
        "Coq"
      ]
    },
    {
      "name": "Other Ops/Infrastructure",
      "level": "Some experience",
      "keywords": [
        "Docker",
        "Bamboo",
				"Kubernetes",
				"Jenkins"
      ]
    },
    {
      "name": "Data science / Numerical Simulations",
      "level": "Some experience",
      "keywords": [
        "Monte Carlo simulation",
        "Genetic algorithms",
        "Parametrizing data sets"
      ]
    }
  ],
  "languages": [
    {
      "language": "Portuguese",
      "fluency": "Native fluency"
    },
    {
      "language": "French",
      "fluency": "Professional proficiency"
    }
  ],
  "interests": [
    {
      "name": "Cycling",
      "keywords": [
        "Crits",
        "Cyclocross"
      ]
    },
    {
      "name": "Art",
      "keywords": [
        "Theatre",
        "20th century literature",
        "Magritte"
      ]
    },
    {
      "name": "Recreational Math",
      "keywords": [
        "ProjectEuler (solved 72 problems using Haskell)"
      ]
    },
		{
			"name": "Volunteering",
			"keywords": [
				"Reading to preschoolers"
			]
		}
  ]
}
